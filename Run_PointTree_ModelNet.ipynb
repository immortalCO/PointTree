{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[Release]Run_PointTree_ModelNet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4lmXaxD5ynlo",
        "oLvduaqgSRuq",
        "rqM9d9ravexM",
        "oiILxo2qlfBf",
        "EuxwBHjYRiKZ",
        "IbtQ22wxRlMB"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lmXaxD5ynlo"
      },
      "source": [
        "# **Starting Work**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7mHJSTOLfAk"
      },
      "source": [
        "from os import chdir, environ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global logging_init_flag\n",
        "logging_init_flag = False\n",
        "\n",
        "def init_logging(OUTPUT):\n",
        "    global logging_init_flag\n",
        "    if logging_init_flag:\n",
        "        return\n",
        "    logging_init_flag = True\n",
        "\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    formatter = logging.Formatter(\n",
        "        '%(asctime)s - %(levelname)s:\\t%(message)s',\n",
        "        datefmt='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    fh = logging.FileHandler(f\"{OUTPUT}/training.log\")\n",
        "    fh.setLevel(logging.INFO)\n",
        "    fh.setFormatter(formatter)\n",
        "\n",
        "    ch = logging.StreamHandler()\n",
        "    ch.setLevel(logging.DEBUG)\n",
        "    ch.setFormatter(formatter)\n",
        "\n",
        "    logger.addHandler(ch)\n",
        "    logger.addHandler(fh)\n",
        "\n"
      ],
      "metadata": {
        "id": "E1_pFKn6s08r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "id": "LXqkCjoYKaPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHDHoF-Lsmle"
      },
      "source": [
        "!ls -lt --time-style='+%y-%m-%d %H:%M:%S'\n",
        "!dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLvduaqgSRuq"
      },
      "source": [
        "# **Experiment Init**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XInyKWaTSYXY"
      },
      "source": [
        "import json\n",
        "import h5py\n",
        "import torch\n",
        "import os\n",
        "from encoder import *\n",
        "# from encoder import MLP\n",
        "# from encoder_resmlp import *\n",
        "from res_encoder import ResEncoder\n",
        "from dyn_encoder import DynEncoder, SampleEncoder\n",
        "from build_tree import get_directions, init_directions\n",
        "import logging\n",
        "from dataset import *\n",
        "\n",
        "prefix = \"_affine_xinf\"\n",
        "no_prealign = False \n",
        "rotate_only = False\n",
        "test_as_valid = False #False\n",
        "augment = 1\n",
        "transform = affine_transform \n",
        "use_symmetry_loss = False\n",
        "dim = 4096 # 4096\n",
        "lowrk = 999999999\n",
        "dim_layer0 = 32 # 32\n",
        "dim_repeat_cut = 5 # 5\n",
        "inner_dim = 64\n",
        "train_augment = False\n",
        "permute_augment = False #True\n",
        "model_size = 2 ** 11\n",
        "num_layers = 1 \n",
        "num_res_layers = 1\n",
        "carry_dim = dim_layer0 * 8\n",
        "upload_remove_equal = False # False\n",
        "use_small_point2embed = False\n",
        "model_name = 'ResEncoder' \n",
        "pca_augment = False # True\n",
        "augment_fn = lambda pts : (pts, None) #augment_generator(pts, dropout=0.875, shift=True, jitter=True, scale=True)\n",
        "disable_alignment = False\n",
        "extra_sample_coef = 1/9\n",
        "coo_dim = 4\n",
        "catmlp = False\n",
        "use_norm = False\n",
        "complete_dataset = False\n",
        "augment_fn = lambda pts : (pts, None) #augment_generator(pts, dropout=0.875, shift=True, jitter=True, scale=True)\n",
        "\n",
        "\n",
        "# prefix = \"_affine_pdnet_nopa_x4\"\n",
        "# no_prealign = True \n",
        "# rotate_only = False\n",
        "# test_as_valid = False\n",
        "# augment = 4\n",
        "# transform = affine_transform \n",
        "# use_symmetry_loss = False\n",
        "# dim = 1024 # 4096\n",
        "# lowrk = 999999999\n",
        "# dim_layer0 = 8 # 32\n",
        "# dim_repeat_cut = 5 # 5\n",
        "# train_augment = False\n",
        "# permute_augment = False #True\n",
        "# model_size = 2 ** 11\n",
        "# num_layers = 1\n",
        "# num_res_layers = 1\n",
        "# carry_dim = dim_layer0 * 8\n",
        "# upload_remove_equal = False # False\n",
        "# use_small_point2embed = False\n",
        "# model_name = 'Encoder' \n",
        "# pca_augment = False # True\n",
        "# augment_fn = lambda x : x\n",
        "# disable_alignment = False\n",
        "# catmlp = True\n",
        "# use_norm = False\n",
        "# complete_dataset = False\n",
        "# augment_fn = lambda pts : (pts, None) #augment_generator(pts, dropout=0.875, shift=True, jitter=True, scale=True)\n",
        "\n",
        "# prefix = \"_orig_cpp_tav_complete_norm_xinf\"\n",
        "# no_prealign = True \n",
        "# rotate_only = False\n",
        "# use_norm = True\n",
        "# complete_dataset = True\n",
        "# test_as_valid = True\n",
        "# augment = 1\n",
        "# transform = lambda x : x\n",
        "# use_symmetry_loss = True\n",
        "# dim = 4096 # 4096\n",
        "# lowrk = 999999999\n",
        "# dim_layer0 = 32 # 32\n",
        "# dim_repeat_cut = 5 # 5\n",
        "# train_augment = False\n",
        "# permute_augment = False #True\n",
        "# model_size = 2 ** 11\n",
        "# num_layers = 1\n",
        "# num_res_layers = 1\n",
        "# carry_dim = dim_layer0 * 8\n",
        "# upload_remove_equal = False # False\n",
        "# use_small_point2embed = False\n",
        "# model_name = 'Encoder' \n",
        "# pca_augment = False # True\n",
        "# # augment_fn = lambda pts : augment_generator(pts, fetch_perm=use_norm, dropout=0.5, shift=True, jitter=True, scale=True, agg_coef=0.5) # dropout=0.875\n",
        "# augment_fn = lambda pts : augment_generator(pts, fetch_perm=use_norm) #, fetch_perm=use_norm, shift=True, scale=True)\n",
        "# disable_alignment = False\n",
        "# catmlp = False\n",
        "\n",
        "# affine new\n",
        "# prefix = \"_affine_dyn_xinf\"\n",
        "# no_prealign = False \n",
        "# rotate_only = False\n",
        "# test_as_valid = False\n",
        "# augment = 1\n",
        "# transform = affine_transform \n",
        "# use_symmetry_loss = False\n",
        "# dim = 1024 # 4096\n",
        "# lowrk = 999999999\n",
        "# dim_layer0 = 32 # 32\n",
        "# dim_repeat_cut = 0 # 5\n",
        "# inner_dim = 64\n",
        "# train_augment = False\n",
        "# permute_augment = False #True\n",
        "# model_size = 2 ** 11\n",
        "# num_layers = 1 \n",
        "# num_res_layers = 1\n",
        "# carry_dim = dim_layer0 * 8\n",
        "# upload_remove_equal = False # False\n",
        "# use_small_point2embed = False\n",
        "# model_name = 'SampleEncoder' \n",
        "# pca_augment = False # True\n",
        "# augment_fn = lambda pts : (pts, None) #augment_generator(pts, dropout=0.875, shift=True, jitter=True, scale=True)\n",
        "# disable_alignment = False\n",
        "# extra_sample_coef = 1/9\n",
        "# coo_dim = 4\n",
        "\n",
        "\n",
        "# orig\n",
        "# prefix = \"_orig_cpp_tav_xinf\"\n",
        "# no_prealign = True \n",
        "# rotate_only = False\n",
        "# test_as_valid = True\n",
        "# augment = 1\n",
        "# transform = lambda x : x\n",
        "# use_symmetry_loss = False\n",
        "# dim = 4096 # 4096\n",
        "# lowrk = 999999999\n",
        "# dim_layer0 = 32 # 32\n",
        "# dim_repeat_cut = 5 # 5\n",
        "# train_augment = False\n",
        "# permute_augment = False #True\n",
        "# model_size = 2 ** 11\n",
        "# num_layers = 1\n",
        "# num_res_layers = 1\n",
        "# carry_dim = dim_layer0 * 8\n",
        "# upload_remove_equal = False # False\n",
        "# use_small_point2embed = False\n",
        "# model_name = 'Encoder' \n",
        "# pca_augment = False # True\n",
        "# augment_fn = lambda pts : augment_generator(pts, dropout=0.875, shift=True, jitter=True, scale=True, rotate_yaxis=False)\n",
        "# disable_alignment = False\n",
        "\n",
        "\n",
        "OUTPUT = 'scratch'\n",
        "\n",
        "sample_layers = 50 # 2\n",
        "channel = 1\n",
        "\n",
        "\n",
        "sample_child_first = False # True in l7s1\n",
        "num_classes = 40\n",
        "DATASET = './datasets/ModelNet' if complete_dataset else './datasets/ModelNet40'\n",
        "chaos_limit = 0\n",
        "\n",
        "init_logging(OUTPUT)\n",
        "logging.info(f\"prefix = {prefix}\")\n",
        "_ = init_directions(chaos_limit, calc_dmap=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ve1aug-tytB"
      },
      "source": [
        "torch.manual_seed(674433238)\n",
        "has_align_feature = False\n",
        "if model_name == 'Encoder':\n",
        "    model = Encoder(model_size, sample_layers, dim, OUTPUT, catmlp=catmlp, extra_dim=3 if use_norm else 0, channel=channel, sample_child_first=sample_child_first, dim_layer0=dim_layer0, dim_repeat_cut=dim_repeat_cut, use_symmetry_loss=use_symmetry_loss).cuda()\n",
        "elif model_name == 'ResEncoder':\n",
        "    has_align_feature = True\n",
        "    model = ResEncoder(num_res_layers, carry_dim, model_size, sample_layers, dim, OUTPUT, catmlp=catmlp, extra_dim=3 if use_norm else 0, channel=channel, sample_child_first=sample_child_first, dim_layer0=dim_layer0, dim_repeat_cut=dim_repeat_cut, use_symmetry_loss=use_symmetry_loss).cuda()\n",
        "elif model_name == 'EncoderKdtAlign':\n",
        "    has_align_feature = True\n",
        "    model = EncoderKdtAlign(model_size, sample_layers, dim, OUTPUT, extra_dim=3 if use_norm else 0, num_layers=num_layers, channel=channel, sample_child_first=sample_child_first, dim_layer0=dim_layer0, dim_repeat_cut=dim_repeat_cut, use_symmetry_loss=use_symmetry_loss).cuda()\n",
        "elif model_name == 'EncoderRec':\n",
        "    model = EncoderRec(model_size, sample_layers, dim, OUTPUT, extra_dim=3 if use_norm else 0, num_layers=2, channel=channel, sample_child_first=sample_child_first, dim_layer0=dim_layer0, dim_repeat_cut=dim_repeat_cut, use_symmetry_loss=use_symmetry_loss).cuda()\n",
        "elif model_name == 'DynEncoder':\n",
        "    has_align_feature = True\n",
        "    model = DynEncoder(model_size, OUTPUT=OUTPUT, extra_dim=3 if use_norm else 0, num_layer=num_layers, dim=dim, inner_dim=inner_dim, dim_layer0=dim_layer0, dim_repeat_cut=dim_repeat_cut, coo_dim=coo_dim, use_symmetry_loss=use_symmetry_loss).cuda()\n",
        "elif model_name == 'SampleEncoder':\n",
        "    has_align_feature = True\n",
        "    encoder = lambda : Encoder(model_size, sample_layers, dim, OUTPUT, extra_dim=3 if use_norm else 0, channel=channel, sample_child_first=sample_child_first, dim_layer0=dim_layer0, dim_repeat_cut=dim_repeat_cut, use_symmetry_loss=use_symmetry_loss).cuda()\n",
        "    sample_encoder = lambda : Encoder(model_size * 4, sample_layers, dim * 2, OUTPUT, extra_dim=3 if use_norm else 0, point_dim=inner_dim, channel=channel, sample_child_first=sample_child_first, dim_layer0=dim_layer0, dim_repeat_cut=dim_repeat_cut, use_symmetry_loss=use_symmetry_loss).cuda()\n",
        "    # sample_encoder = Encoder(model_size, sample_layers, dim, OUTPUT, extra_dim=3 if use_norm else 0, point_dim=inner_dim, channel=channel, sample_child_first=sample_child_first, dim_layer0=dim_layer0, dim_repeat_cut=dim_repeat_cut, use_symmetry_loss=use_symmetry_loss).cuda()\n",
        "    model = SampleEncoder(encoder, sample_encoder, extra_sample_coef=extra_sample_coef).cuda()\n",
        "else:\n",
        "    assert False, f\"Unsupported {model_name}\"\n",
        "model.dim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hack(model):\n",
        "    if upload_remove_equal:\n",
        "        for i, layer in enumerate(model.layers):\n",
        "            if layer.idim == layer.odim:\n",
        "                layer.upload = torch.nn.Identity().cuda()\n",
        "                print(f\"#{i}: Set upload to identity.\")\n",
        "    if lowrk < dim:\n",
        "        from encoder import convert_lowrk\n",
        "        # pts_align = model.layers[0].pts_align\n",
        "        # for i in range(6):\n",
        "        #     pts_align.fc[i] = convert_lowrk(pts_align.fc[i], lowrk).cuda()\n",
        "        # for l in model.layers[0].mlp.layers:\n",
        "        #     l.linear = convert_lowrk(l.linear, lowrk).cuda()\n",
        "        for i, layer in enumerate(model.layers):\n",
        "            if layer.layer_type != 'leaf' and isinstance(layer.upload, MLP):\n",
        "                for l in layer.upload.layers:\n",
        "                    l.linear = convert_lowrk(l.linear, lowrk).cuda()\n",
        "                print(f\"#{i}: Converted to lowrk.\")\n",
        "    if use_small_point2embed:\n",
        "        l = model.layers[0]\n",
        "        l.mlp = MLP([l.idim, l.odim * 4, l.odim], init=0.25).cuda()\n",
        "        print(f\"Reset middle layer of point2embed\")\n",
        "\n",
        "def hack_seg(model):\n",
        "    hack(model.encoder)\n",
        "\n",
        "if isinstance(model, Encoder):\n",
        "    hack(model)\n",
        "elif isinstance(model, EncoderKdtAlign):\n",
        "    hack(model.encoder)\n",
        "    upload_remove_equal = True\n",
        "    use_small_point2embed = True\n",
        "\n",
        "    def hack_rec(model):\n",
        "        if isinstance(model, Encoder):\n",
        "            hack(model)\n",
        "        else:\n",
        "            hack(model.encoder)\n",
        "            hack_rec(model.align.encoder)\n",
        "\n",
        "    hack_rec(model.align.encoder)\n",
        "elif isinstance(model, EncoderRec):\n",
        "    hack(model.encoder)\n",
        "elif isinstance(model, ResEncoder):\n",
        "    print(\"feed:\")\n",
        "    hack(model.feed)\n",
        "    # upload_remove_equal = True\n",
        "    # use_small_point2embed = True\n",
        "    print(\"layers:\")\n",
        "    for seg in model.layers:\n",
        "        hack_seg(seg)\n",
        "else:\n",
        "    print(\"Unsupported\")\n",
        "    # assert False, \"Unsupported\""
      ],
      "metadata": {
        "id": "7z32SzUQT1JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    if disable_alignment:\n",
        "        model.layers[0].pts_align = torch.nn.Identity()\n",
        "        print(\"alignment disabled\")\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "XBZS-QsUjjOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "count"
      ],
      "metadata": {
        "id": "U5mTe-nOcimO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS58zLcFNjT9"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat build_tree.py | grep cvg"
      ],
      "metadata": {
        "id": "0ZQqTIoV4VuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyuWtrElslel"
      },
      "source": [
        "assert False, \"debug part cannot be run through\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.tree.record_vec"
      ],
      "metadata": {
        "id": "DXmmxEEV4VF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OH3DJPZwOvss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU1ibEuOnAei"
      },
      "source": [
        "from load_modelnet import load_modelnet\n",
        "k = 88\n",
        "pts = h5py.File(f'./datasets/ShapeNetPart/train0.h5')['data'][k]\n",
        "pts = torch.tensor(pts)\n",
        "# pts, labels = load_modelnet(max_count=1, num_workers=1)\n",
        "# pts = pts[0][:2048]\n",
        "output = None\n",
        "h5py.File(f'./datasets/ShapeNetPart/train0.h5')['label'][k]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = []\n",
        "d = h5py.File(f'./datasets/ShapeNetPart/train0.h5')['label']\n",
        "for i in range(2048):\n",
        "    if d[i][0] == 8:\n",
        "        a.append(str(i))\n",
        "print(\",\".join(a))"
      ],
      "metadata": {
        "id": "Lz7E9EtwOxAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.tree.use_sym"
      ],
      "metadata": {
        "id": "5t_Cd7Ql5UfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_pts(pts, file):\n",
        "    pts -= pts.mean(dim=0)\n",
        "    pts /= pts.norm(dim=-1).mean()\n",
        "    with open(f\"{file}.txt\", 'w') as file:\n",
        "        file.write(f\"{pts.shape[0]}\\n\")\n",
        "        for p in pts:\n",
        "            file.write(\"%.8lf %.8lf %.8lf\\n\" % (p[0].item(), p[1].item(), p[2].item()))\n",
        "\n",
        "def read_pts(file):\n",
        "    pts = []\n",
        "    with open(f\"{file}.txt\") as file:\n",
        "        for line in file:\n",
        "            line = line.split()\n",
        "            if len(line) == 3:\n",
        "                pts.append(list(map(float, line)))\n",
        "    return torch.tensor(pts)"
      ],
      "metadata": {
        "id": "EVVOk6-_Bzu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZON2cTGeAKW"
      },
      "source": [
        "pts = transform(pts)\n",
        "# pts = affine_lim(torch.pi / 4)(pts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pts, _ = augment_fn(pts)\n",
        "#write_pts(pts, \"/mnt/c/Users/immor/OneDrive/Academic/PointCloud/tree_builder_cpp/pts_debug\")"
      ],
      "metadata": {
        "id": "i_P4ZbhYLOVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoy81cu7nxdj"
      },
      "source": [
        "tree = model.tree\n",
        "pts, output, extra = model.tree.arrange(pts, rotate=not no_prealign, extra=False, pca=True,\n",
        "                                          rotate_only=rotate_only, debug=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, l in enumerate(output):\n",
        "    print(i, l.shape)"
      ],
      "metadata": {
        "id": "1WfNFA-n99gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86L-VbTUqyGo"
      },
      "source": [
        "batch_size = 32\n",
        "model.cache = True\n",
        "model.train()\n",
        "_pts = pts.unsqueeze(0).expand(batch_size, *pts.shape)\n",
        "_output = list(map(lambda x : x.unsqueeze(0).expand(batch_size, *x.shape), output))\n",
        "_extra = extra.unsqueeze(0).expand(batch_size, *extra.shape)\n",
        "for _ in range(8):\n",
        "    model.debug = True\n",
        "    feature = model(_pts, _output, _extra, perm=0)\n",
        "    print(feature.shape)\n",
        "    (feature.sum()).backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output[2].shape"
      ],
      "metadata": {
        "id": "MsixK5yN4tjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree.use_sym = False"
      ],
      "metadata": {
        "id": "bNa8_r3I4lDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-t36sKk4VIkr"
      },
      "source": [
        "%matplotlib inline\n",
        "def plot(points, color_layers=2, output=None, recenter=True):\n",
        "    import matplotlib.pyplot as plt\n",
        "    from mpl_toolkits.mplot3d import Axes3D\n",
        "    import matplotlib.gridspec as gridspec\n",
        "    import numpy as np\n",
        "\n",
        "    points = points.clone().cpu().numpy()\n",
        "    x, y, z = points[:,0], points[:,1], points[:,2]\n",
        "    if recenter:\n",
        "        x -= x.mean()\n",
        "        y -= y.mean()\n",
        "        z -= z.mean()\n",
        "\n",
        "    color = np.array([0 for _ in range(x.shape[0])])\n",
        "    \n",
        "    def mark_color(z, last_ind=None):\n",
        "        n = z.shape[0]\n",
        "        ind = torch.tensor(z).sort()[1].numpy()\n",
        "        indl, indr = ind[: n >> 1], ind[n >> 1 : ]\n",
        "        if last_ind is not None:\n",
        "            indl, indr = last_ind[indl], last_ind[indr]\n",
        "        color[indr] += 1\n",
        "        return indl, indr\n",
        "\n",
        "    if output is not None:\n",
        "\n",
        "        color = torch.tensor([0]).cuda()\n",
        "        # print(tree.layer_size)\n",
        "        if  model_name in ['ResEncoder', 'DynEncoder']:\n",
        "            layers = model.feed.layers\n",
        "        elif model_name in ['SampleEncoder']:\n",
        "            layers = model.encoder.encoder.layers\n",
        "        else:\n",
        "            layers = model.layers\n",
        "        for i, (layer, next_layer_size) in enumerate(zip(reversed(layers[1:]), tree.layer_size[1:])):\n",
        "            next_color = torch.zeros(next_layer_size).long().cuda()\n",
        "            if i < color_layers:\n",
        "                color = color << 1\n",
        "                right_add = 1\n",
        "            else:\n",
        "                right_add = 0\n",
        "            assert layer.child_l.max() < next_layer_size\n",
        "            assert layer.child_r.max() < next_layer_size\n",
        "            next_color[layer.child_l] = color\n",
        "            next_color[layer.child_r] = color + right_add\n",
        "            color = next_color\n",
        "\n",
        "        arrange = output[0]\n",
        "        if len(arrange.shape) > 1:\n",
        "            arrange = arrange[0]\n",
        "        n = points.shape[0]\n",
        "        # print(color)\n",
        "        color = np.array([color[arrange == i].max().item() for i in range(n)])\n",
        "\n",
        "    elif color_layers >= 1:\n",
        "        l, r = mark_color(z)\n",
        "        if color_layers >= 2:\n",
        "            color *= 2\n",
        "            ll, lr = mark_color(y[l], l)\n",
        "            rl, rr = mark_color(y[r], r)\n",
        "            if color_layers >= 3:\n",
        "                color *= 2\n",
        "                for p in [ll, lr, rl, rr]:\n",
        "                    mark_color(x[p], p)\n",
        "\n",
        "    colormap = np.array(['red', 'blue', 'green', 'yellow', 'grey', 'orange', 'purple', 'cyan'])\n",
        "    import random\n",
        "    #random.shuffle(colormap)\n",
        "\n",
        "    fig = plt.figure(dpi=80)\n",
        "    gs = gridspec.GridSpec(nrows=2, ncols=4, left=0.1, right=2.5, wspace=0.05, hspace=0.1, bottom=0.1, top=1.3)\n",
        "    for i in range(8):\n",
        "        ax = fig.add_subplot(gs[i // 4, i % 4], projection='3d')\n",
        "\n",
        "        x, y, z = points[:,0], points[:,1], points[:,2]\n",
        "        labx, laby, labz = 'x', 'y', 'z'\n",
        "        if (i & 4) != 0:    x = -x; labx = '-x'\n",
        "        if (i & 2) != 0:    y = -y; laby = '-y'\n",
        "        if (i & 1) != 0:    z = -z; labz = '-z'\n",
        "\n",
        "        lmin = min(x.min(), y.min(), z.min())\n",
        "        lmax = max(x.max(), y.max(), z.max())\n",
        "        ax.scatter(x, y, z, c=colormap[color], marker='.')\n",
        "\n",
        "        ax.set_xlim(lmin, lmax)\n",
        "        ax.set_ylim(lmin, lmax)\n",
        "        ax.set_zlim(lmin, lmax)\n",
        "\n",
        "        ax.set_xlabel(labx)\n",
        "        ax.set_ylabel(laby)\n",
        "        ax.set_zlabel(labz) \n",
        "    plt.show()\n",
        "\n",
        "    fig = plt.figure(dpi=80)\n",
        "    ncolor = color.max() + 1\n",
        "    nrows = max(1, ncolor // 4)\n",
        "    ncols = min(ncolor, 4)\n",
        "    gs = gridspec.GridSpec(nrows=nrows, ncols=ncols, left=0.1, right=0.1 + 0.6 * ncols, wspace=0.05, hspace=0.05, bottom=0.1, top=0.1 + 0.6 * nrows)\n",
        "    for i in range(ncolor):\n",
        "        ax = fig.add_subplot(gs[i // 4, i % 4], projection='3d')\n",
        "\n",
        "        p = points[color == i]\n",
        "        x, y, z = p[:,0], p[:,1], p[:,2]\n",
        "\n",
        "        lmin = min(x.min(), y.min(), z.min())\n",
        "        lmax = max(x.max(), y.max(), z.max())\n",
        "        ax.scatter(x, y, z, c=colormap[i], marker='.')\n",
        "\n",
        "        ax.set_xlim(lmin, lmax)\n",
        "        ax.set_ylim(lmin, lmax)\n",
        "        ax.set_zlim(lmin, lmax)\n",
        "\n",
        "        ax.set_xlabel('x')\n",
        "        ax.set_ylabel('y')\n",
        "        ax.set_zlabel('z') \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = None"
      ],
      "metadata": {
        "id": "v1LJAurX4_gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEEZykGAVjLk"
      },
      "source": [
        "plot(pts.squeeze(), 3, output=output, recenter=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(torch.pca_lowrank(pts)[0], 3, output=output, recenter=False)"
      ],
      "metadata": {
        "id": "39lDcNa_Md1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(12):\n",
        "    pts = h5py.File(f'{DATASET}/train0.h5')['data'][i]\n",
        "    pts = torch.tensor(pts)\n",
        "    output = None\n",
        "    print(i)\n",
        "    tree = model.tree\n",
        "    tree.use_sym = True\n",
        "    pts, output, extra = model.tree.arrange(pts, rotate=not no_prealign, extra=False, pca=True,\n",
        "                                          rotate_only=rotate_only, debug=False)\n",
        "    plot(pts.squeeze(), 3, output=output, recenter=False)\n"
      ],
      "metadata": {
        "id": "RNAQtGGBHHNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(12):\n",
        "    pts = h5py.File(f'{DATASET}/train0.h5')['data'][i]\n",
        "    pts = torch.tensor(pts)\n",
        "    output = None\n",
        "    print(i)\n",
        "    tree = model.tree\n",
        "    tree.use_sym = False\n",
        "    pts, output, extra = model.tree.arrange(pts, rotate=not no_prealign, extra=False, pca=True,\n",
        "                                          rotate_only=rotate_only, debug=False)\n",
        "    plot(pts.squeeze(), 3, output=output, recenter=False)\n"
      ],
      "metadata": {
        "id": "n3i2ayDYBqLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(read_pts(\"/mnt/c/Users/immor/OneDrive/Academic/PointCloud/tree_builder_cpp/pts_debug\"), 3, output=output, recenter=False)"
      ],
      "metadata": {
        "id": "dejLDQ0nwkoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.align_dim"
      ],
      "metadata": {
        "id": "iJhghxnEQr7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(f'{OUTPUT}/trained_best{prefix}.pth')['encoder'])"
      ],
      "metadata": {
        "id": "6GHd2Fxm-mvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.extra_sample_coef = 0.1"
      ],
      "metadata": {
        "id": "m3CVzYWheKH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output[0]"
      ],
      "metadata": {
        "id": "796LZUI8_ivU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.cached_inputs[0][0][0]"
      ],
      "metadata": {
        "id": "3tlWRGZt_eIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(pts.squeeze(), 3, output=model.cached_inputs[0])\n",
        "plot(pts.squeeze(), 3, output=model.cached_inputs[1])\n",
        "plot(pts.squeeze(), 3, output=model.cached_inputs[2])"
      ],
      "metadata": {
        "id": "IDW28c-l-_mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_tri(pts, tri_ind)"
      ],
      "metadata": {
        "id": "rk52R7MBIve9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_tri(points, tri):\n",
        "    points = points.numpy()\n",
        "    tri = tri.numpy()\n",
        "\n",
        "    fig = plt.figure(dpi=200)\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    \n",
        "    edges = collect_edges(tri)\n",
        "    x = np.array([])\n",
        "    y = np.array([])\n",
        "    z = np.array([])\n",
        "    for (i,j) in edges:\n",
        "        x = np.append(x, [points[i, 0], points[j, 0], np.nan])      \n",
        "        y = np.append(y, [points[i, 1], points[j, 1], np.nan])      \n",
        "        z = np.append(z, [points[i, 2], points[j, 2], np.nan])\n",
        "    ax.plot3D(x, y, z, color='g', lw='0.1')\n",
        "\n",
        "    ax.scatter(points[:,0], points[:,1], points[:,2], color='b', s=1)\n",
        "\n",
        "\n",
        "def collect_edges(tri):\n",
        "    edges = set()\n",
        "\n",
        "    def sorted_tuple(a,b):\n",
        "        return (a,b) if a < b else (b,a)\n",
        "    # Add edges of tetrahedron (sorted so we don't add an edge twice, even if it comes in reverse order).\n",
        "    for (i0, i1, i2, i3) in tri:\n",
        "        edges.add(sorted_tuple(i0,i1))\n",
        "        edges.add(sorted_tuple(i0,i2))\n",
        "        edges.add(sorted_tuple(i0,i3))\n",
        "        edges.add(sorted_tuple(i1,i2))\n",
        "        edges.add(sorted_tuple(i1,i3))\n",
        "        edges.add(sorted_tuple(i2,i3))\n",
        "    return edges"
      ],
      "metadata": {
        "id": "tUpzwbFoIWqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp7w45moXheA"
      },
      "source": [
        "pts = train_data[7][0]\n",
        "output = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwnsH3fVEy8E"
      },
      "source": [
        "for i in range(50):\n",
        "    output, arrange = tree.arrange(pts, basic=basic, debug=False, device='cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfF1uQMf6E4V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5J5eeH4A_KYE"
      },
      "source": [
        "feature.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emzothiutr1R"
      },
      "source": [
        "for i in range(10):\n",
        "    loss = model(pts.unsqueeze(0), list(map(lambda x : x.unsqueeze(0), output))).view(-1).sum()\n",
        "    loss.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjWu4jsiezyq"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4IW-IhaXNI7"
      },
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(name, param.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3m193YMMKJX"
      },
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if 'activate' in name:\n",
        "        print(name, \"\\t\", param.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edrzFkxkfSnH"
      },
      "source": [
        "!cp /tmp/cppinput.txt scratch/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTdWpWCrfHto"
      },
      "source": [
        "!tail -n 5 /tmp/cppoutput.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqM9d9ravexM"
      },
      "source": [
        "# **Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWRjuhs4tl1v"
      },
      "source": [
        "from dataset import *\n",
        "import numpy as np\n",
        "from load_modelnet import load_modelnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.multiprocessing.set_sharing_strategy('file_system')"
      ],
      "metadata": {
        "id": "IgGbZjr1iGT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3Tua4uPKA8G"
      },
      "source": [
        "if False:\n",
        "    from random import shuffle, seed\n",
        "    raw_data = torch.load(f'{OUTPUT}/train_data{prefix}.pth')\n",
        "    n = len(raw_data) // data_augments\n",
        "    perm = list(range(n))\n",
        "    seed(674433238)\n",
        "    shuffle(perm)\n",
        "    train_perm = perm[: -n//8]\n",
        "    valid_perm = perm[-n//8: ]\n",
        "    torch.save([train_perm, valid_perm], f'{OUTPUT}/train_split{prefix}.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqqwreNL5L6g"
      },
      "source": [
        "make = make_data_default\n",
        "if rotate_only:\n",
        "    make = make_data_rotate_only\n",
        "if no_prealign:\n",
        "    make = make_data_no_prealign"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG7rbo4ekcjE"
      },
      "source": [
        "if complete_dataset:\n",
        "    clouds, labels = load_modelnet(folder=DATASET, modelnet='modelnet40', split='train',  num_workers=16)\n",
        "else:\n",
        "    n_train = 5\n",
        "    clouds = []\n",
        "    labels = []\n",
        "    for i in range(n_train):\n",
        "        data_file = h5py.File(f'{DATASET}/train{i}.h5')\n",
        "        clouds.append(torch.tensor(np.array(data_file['data'])))\n",
        "        labels.append(torch.tensor(np.array(data_file['label'])))\n",
        "\n",
        "    clouds = torch.cat(clouds, dim=0)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "\n",
        "if test_as_valid:\n",
        "    train_dataset = PointCloudDataset(clouds, labels, model.tree.arrange, augment=augment, transform=transform, make=make, augment_fn=augment_fn, use_norm=use_norm)\n",
        "else:\n",
        "    try:\n",
        "        train_perm, valid_perm = torch.load(f'{OUTPUT}/train_split{prefix}.pth')\n",
        "    except:\n",
        "        train_perm, valid_perm = torch.load(f'{OUTPUT}/train_split.pth')\n",
        "\n",
        "    from math import ceil\n",
        "    train_dataset = PointCloudDataset(clouds, labels, model.tree.arrange, augment=augment, transform=transform, make=make, subset=train_perm, augment_fn=augment_fn, use_norm=use_norm) #, extra_labels=clouds)\n",
        "    valid_dataset = PointCloudDataset(clouds, labels, model.tree.arrange, augment=1, transform=transform, make=make, subset=valid_perm, use_norm=use_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3j6QUPo11OV"
      },
      "source": [
        "if complete_dataset:\n",
        "    clouds, labels = load_modelnet(folder=DATASET, modelnet='modelnet40', split='test', num_workers=16)\n",
        "else:\n",
        "    n_test = 2\n",
        "    clouds = []\n",
        "    labels = []\n",
        "    for i in range(n_test):\n",
        "        data_file = h5py.File(f'{DATASET}/test{i}.h5')\n",
        "        clouds.append(torch.tensor(np.array(data_file['data'])))\n",
        "        labels.append(torch.tensor(np.array(data_file['label'])))\n",
        "\n",
        "    clouds = torch.cat(clouds, dim=0)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "\n",
        "test_dataset = PointCloudDataset(clouds, labels, model.tree.arrange, augment=1, make=make, transform=transform, use_norm=use_norm)\n",
        "\n",
        "if test_as_valid:\n",
        "    valid_dataset = test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX9UCfSsTweO"
      },
      "source": [
        "len(train_dataset), len(valid_dataset), len(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfWwHySFzgbz"
      },
      "source": [
        "target = [] # ['valid', 'test'], ['train', 'valid', 'test']\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "for name, dataset in zip(['train', 'valid', 'test'], [train_dataset, valid_dataset, test_dataset]):\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    if name not in target:\n",
        "        continue\n",
        "\n",
        "    num_workers = 16\n",
        "    batch_size = 8\n",
        "\n",
        "    data_init = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, \n",
        "                                            num_workers=num_workers, collate_fn=placeholder, pin_memory=False, drop_last=False)\n",
        "    logging.info(f\"Init {name}\")\n",
        "\n",
        "    counter = 0\n",
        "    mem = []\n",
        "\n",
        "    for i, data in enumerate(tqdm(data_init)):\n",
        "        mem += data\n",
        "        # logging.debug(f\"Init {name}: {i+1}/{len(data_init)}\")\n",
        "\n",
        "        if len(mem) >= 2048:\n",
        "            torch.save(mem, f'{OUTPUT}/{name}_data{prefix}.{counter}.pth')\n",
        "            counter += 1\n",
        "            mem.clear()\n",
        "            gc.collect()\n",
        "            \n",
        "    torch.save(mem, f'{OUTPUT}/{name}_data{prefix}.{counter}.pth')\n",
        "    del mem\n",
        "    gc.collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiILxo2qlfBf"
      },
      "source": [
        "# **Train**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defs"
      ],
      "metadata": {
        "id": "EuxwBHjYRiKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.multiprocessing.set_sharing_strategy('file_descriptor')"
      ],
      "metadata": {
        "id": "OeFNZwUEo8OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw0-l-jDmbMO"
      },
      "source": [
        "from dataset import make_batch_train, make_batch_eval\n",
        "\n",
        "global batch_size\n",
        "num_workers = 16\n",
        "batch_size = 128\n",
        "\n",
        "for name, dataset in zip(['train', 'valid', 'test'], [train_dataset, valid_dataset, test_dataset]):\n",
        "    print(f\"Loading {name}\")\n",
        "    dataset.mem = None\n",
        "\n",
        "    try:\n",
        "        tmp = torch.load(f'{OUTPUT}/{name}_data{prefix}.pth')\n",
        "        dataset.mem = tmp\n",
        "        assert len(dataset.mem) == len(dataset), f'Size unmatch: {len(dataset.mem)} != {len(dataset)}'\n",
        "        num_workers = 1\n",
        "        continue\n",
        "    except:\n",
        "        print(\"Try part mode\")\n",
        "\n",
        "    dataset.mem = []\n",
        "    for i in range(0, 1000000000):\n",
        "        try:\n",
        "            tmp = torch.load(f'{OUTPUT}/{name}_data{prefix}.{i}.pth')\n",
        "            dataset.mem += tmp\n",
        "            print(f\"Loaded part {i} # = {len(tmp)}\")\n",
        "        except:\n",
        "            break\n",
        "    if len(dataset.mem) > 0:\n",
        "        num_workers = 1\n",
        "        continue\n",
        "\n",
        "    print(\"Use force online\")\n",
        "    dataset.mem = None\n",
        "    dataset.force_online = True\n",
        "\n",
        "    \n",
        "mbtrain = make_batch_generator(pca_augment=pca_augment and not no_prealign) #make_batch_train\n",
        "\n",
        "\n",
        "def inf_iter(a):\n",
        "    while True:\n",
        "        for k in a:\n",
        "            yield(k)\n",
        "\n",
        "num_workers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=mbtrain, pin_memory=True, drop_last=True, prefetch_factor=8 if num_workers>1 else 2)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=make_batch_eval, pin_memory=True, drop_last=False, prefetch_factor=8 if num_workers>1 else 2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=make_batch_eval, pin_memory=True, drop_last=False, prefetch_factor=8 if num_workers>1 else 2)\n"
      ],
      "metadata": {
        "id": "M2F3rxpzj2y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIcxQV-Jl-Mf"
      },
      "source": [
        "torch.manual_seed(674433238)\n",
        "\n",
        "class Classifier(torch.nn.Module):\n",
        "    def __init__(self, idim, odim, dropout=0):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "        self.linear1 = make_lowrk(idim, min(idim // 2, 1024), lowrk)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(min(idim // 2, 1024))\n",
        "\n",
        "        # self.linear2 = make_lowrk(idim // 2, idim // 4, lowrk)\n",
        "        # self.bn2 = torch.nn.BatchNorm1d(idim // 4)\n",
        "        self.linear2 = torch.nn.Identity()\n",
        "        self.bn2 = torch.nn.Identity()\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        # self.linear3 = make_lowrk(idim // 4, odim, lowrk)\n",
        "        self.linear3 = make_lowrk(min(idim // 2, 1024), odim, lowrk)\n",
        "\n",
        "    def forward(self, ans):\n",
        "        ans = self.linear1(ans)\n",
        "        ans = self.bn1(ans)\n",
        "        ans = self.relu(ans)\n",
        "        # ans = self.dropout(ans)\n",
        "\n",
        "        ans = self.linear2(ans)\n",
        "        ans = self.bn2(ans)\n",
        "        ans = self.relu(ans)\n",
        "        ans = self.dropout(ans)\n",
        "        \n",
        "        ans = self.linear3(ans)\n",
        "        return ans\n",
        "\n",
        "if 'orig' in prefix:\n",
        "    from encoder import make_lowrk\n",
        "    linear = Classifier(model.dim, num_classes, 1 - 1/2).cuda()\n",
        "\n",
        "    align_num = 1 if has_align_feature else 0\n",
        "    try:\n",
        "        align_num = model.align_num\n",
        "    except:\n",
        "        pass\n",
        "    linear_align = torch.nn.ModuleList([Classifier(model.align_dim, num_classes, 1 - 1/2).cuda() for _ in range(align_num)])\n",
        "    # linear = MLP([model.dim, min(1024, model.dim // 2), min(512, model.dim // 4), num_classes], last_bn=False).cuda()\n",
        "    # linear_align = MLP([model.align_dim, min(1024, model.align_dim // 2), min(512, model.align_dim // 4), num_classes], last_bn=False).cuda() if has_align_feature else torch.nn.Identity().cuda()\n",
        "else:\n",
        "    align_num = 1 if has_align_feature else 0\n",
        "    try:\n",
        "        align_num = model.align_num\n",
        "    except:\n",
        "        pass\n",
        "    linear = MLP([model.dim, min(1024, model.dim // 2), min(512, model.dim // 4), num_classes], last_bn=False).cuda()\n",
        "    linear_align = torch.nn.ModuleList([MLP([model.align_dim, min(1024, model.align_dim // 2), min(512, model.align_dim // 4), num_classes], last_bn=False).cuda() for _ in range(align_num)]) #if has_align_feature else torch.nn.Identity().cuda()\n",
        "    # linear = Classifier(model.dim, num_classes, 1 - 1/4).cuda()\n",
        "    # linear_align =  Classifier(model.dim, num_classes, 1 - 1/4).cuda()\n",
        "\n",
        "print(linear)\n",
        "print(linear_align)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Dj6kfNZmdeq"
      },
      "source": [
        "def evaluate(model, linear, loader, noprint=False, perms=[None]):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    if not noprint:\n",
        "        logging.info(f\"loader # = {len(loader)}\")\n",
        "\n",
        "    print_epoch = 1\n",
        "\n",
        "    model.eval()\n",
        "    linear.eval()\n",
        "    activate = lambda x : x\n",
        "\n",
        "    for epoch, (input, label) in enumerate(loader):\n",
        "        with torch.no_grad():\n",
        "            label = label.squeeze(-1).cuda()\n",
        "            for perm in perms:\n",
        "                result = activate(linear(model(*input, perm=perm).contiguous().view(label.size(0), -1)))\n",
        "\n",
        "                correct += (result.argmax(dim=-1) == label.cuda()).sum().item()\n",
        "                total += label.size()[0]\n",
        "\n",
        "        if not noprint:\n",
        "            if (epoch // batch_size + 1) % print_epoch == 0:\n",
        "                logging.debug(f\"test #{epoch} correct = {'%.6lf' % (correct / total)}\")\n",
        "\n",
        "    if not noprint:\n",
        "        logging.info(f\"Done: score = {'%.8lf' % (correct / total)}\")\n",
        "\n",
        "    model.train()\n",
        "    linear.train()  \n",
        "    \n",
        "    return correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0zAZto5v9DT"
      },
      "source": [
        "from random import choice, randint\n",
        "import build_tree\n",
        "import torch\n",
        "\n",
        "def get_trans(n=3):\n",
        "    if n == 0:  return [None]\n",
        "    return [randint(0, len(build_tree.transforms) - 1) for _ in range(n)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En3eHY5Av_tF"
      },
      "source": [
        "# if 'orig' in prefix:\n",
        "#     ckpt = torch.load(f\"{OUTPUT}/pretrained_affine_pca_nosample_x16_20211207.pth\")\n",
        "#     model.load_state_dict(ckpt['encoder'])\n",
        "#     linear.load_state_dict(ckpt['linear'])\n",
        "#     print(\"Pretrain loaded\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBpg22qOl2Hu"
      },
      "source": [
        "global current_epoch\n",
        "global best_vres\n",
        "current_epoch = 0\n",
        "best_vres = -1.0\n",
        "\n",
        "def train(epoch=100000, lr=1e-4, valid_result_threshold=1.0, mode=1, ckpt=None):\n",
        "    global current_epoch\n",
        "    global best_vres\n",
        "    global batch_size\n",
        "\n",
        "    logging.info(f\"train epoch = {current_epoch + 1} ~ {epoch} threshold = {valid_result_threshold}\")\n",
        "\n",
        "    model.train()\n",
        "    linear.train()\n",
        "    activate = lambda x : x\n",
        "\n",
        "    cum_loss = 0\n",
        "    cum_loss_align = 0\n",
        "\n",
        "    batch_scale = 1 # 1\n",
        "    epoch_scale = 8 # 8\n",
        "    num_trans = 1 if permute_augment else 0\n",
        "\n",
        "\n",
        "    print_epoch = 20\n",
        "    valid_epoch = 20\n",
        "    epoch_since = 0\n",
        "    save_epoch = 100\n",
        "    cut_epoch = 10000000000\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    temperature = 1\n",
        "    threshold = -1.0\n",
        "\n",
        "    crit = torch.nn.CrossEntropyLoss()\n",
        "    crit_align = torch.nn.CrossEntropyLoss()\n",
        "    opt = torch.optim.Adam(list(model.parameters()) + list(linear.parameters()) + list(linear_align.parameters()), lr=lr)\n",
        "\n",
        "    if ckpt is not None:\n",
        "        model.load_state_dict(ckpt['encoder'])\n",
        "        linear.load_state_dict(ckpt['linear'])\n",
        "        linear_align.load_state_dict(ckpt['linear_align'])\n",
        "        opt.load_state_dict(ckpt['opt'])\n",
        "    \n",
        "    warmup = 0\n",
        "    if mode == 1:\n",
        "        sch = torch.optim.lr_scheduler.ExponentialLR(opt, 0.998)\n",
        "    elif mode == 2:\n",
        "        opt.param_groups[0]['lr'] = lr\n",
        "        opt.param_groups[0]['initial_lr'] = lr\n",
        "        sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=10000 // valid_epoch, T_mult=2, eta_min=lr / 100)\n",
        "    elif mode == 3:\n",
        "        lr /= 5\n",
        "        opt.param_groups[0]['lr'] = lr\n",
        "        opt.param_groups[0]['initial_lr'] = lr\n",
        "        sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=10000 // valid_epoch, T_mult=2, eta_min=lr / 100)\n",
        "    else:\n",
        "        assert False\n",
        "\n",
        "\n",
        "    def save(epoch):\n",
        "        torch.save({\n",
        "            'encoder': model.state_dict(),\n",
        "            'linear': linear.state_dict(),\n",
        "            'linear_align': linear_align.state_dict(),\n",
        "            'opt': opt.state_dict(),\n",
        "            'sch': sch.state_dict(),\n",
        "            'best_vres': best_vres,\n",
        "        }, f\"{OUTPUT}/trained_{epoch}.pth\")\n",
        "\n",
        "    if not test_as_valid:\n",
        "        save(current_epoch)\n",
        "\n",
        "    train_iter = inf_iter(train_loader)\n",
        "\n",
        "    for epoch in range(current_epoch + 1, current_epoch + epoch + 1):\n",
        "\n",
        "        current_epoch = epoch\n",
        "\n",
        "        align_coef = 0\n",
        "        align_coef_sum = 1\n",
        "        for _ in range(epoch_scale):\n",
        "            loss = torch.tensor(0.).cuda()\n",
        "            loss_align = torch.tensor(0.).cuda()\n",
        "            for _ in range(batch_scale):\n",
        "                input, label = next(train_iter)\n",
        "                label = label.squeeze(-1).cuda()\n",
        "                for iperm in get_trans(num_trans):\n",
        "                    \n",
        "                    features = model(*input, perm=iperm).reshape(batch_size, -1)\n",
        "                    logits = linear(features)\n",
        "                    loss += crit(logits / temperature, label.cuda())\n",
        "                    if has_align_feature:\n",
        "                        align_coef = 1\n",
        "                        align_coef_sum = 0\n",
        "                        if not isinstance(model.align_feature, list):\n",
        "                            model.align_feature = [model.align_feature]\n",
        "\n",
        "                        for fea, cls in zip(model.align_feature, linear_align):\n",
        "                            align_coef_sum += align_coef\n",
        "                            logits_align = cls(fea)\n",
        "                            loss_align += align_coef * crit_align(logits_align / temperature, label.cuda())\n",
        "                            align_coef *= 0.5\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        correct += (logits.argmax(dim=-1) == label.cuda()).sum().item()\n",
        "                        total += label.size()[0]\n",
        "                        epoch_since += 1\n",
        "            \n",
        "            # assert loss.isnan().sum() == 0\n",
        "            cum_loss += loss.item()\n",
        "            cum_loss_align += loss_align.item()\n",
        "            opt.zero_grad()\n",
        "            # loss.backward()\n",
        "            if 'orig' in prefix:\n",
        "                # (loss / loss.clamp(min=1e-6).item()).backward()\n",
        "                all = (loss / loss.clamp(min=1e-6).item()) + (loss_align / loss_align.clamp(min=1e-6).item())\n",
        "                all.backward()\n",
        "            else:\n",
        "                all = loss + loss_align\n",
        "                all.backward()\n",
        "            opt.step()\n",
        "\n",
        "        if cum_loss / epoch_since < threshold:\n",
        "            epoch_scale, batch_scale = batch_scale, epoch_scale\n",
        "            \n",
        "            logging.info(\"Threshold Reached\")\n",
        "            threshold = -1e10\n",
        "            \n",
        "        if epoch <= 5 or epoch % print_epoch == 0:\n",
        "            valid_str = \"\"\n",
        "            func = logging.debug\n",
        "\n",
        "            stop_training = False\n",
        "            if epoch % valid_epoch == 0:\n",
        "                vres = evaluate(model, linear, valid_loader, noprint=True)\n",
        "                valid_str = f\"valid = {'%.6lf' % vres}\"\n",
        "                stop_training = (vres >= valid_result_threshold)\n",
        "                if vres > best_vres:\n",
        "                    best_vres = vres\n",
        "                    torch.save({\n",
        "                        'encoder': model.state_dict(),\n",
        "                        'linear': linear.state_dict()\n",
        "                    }, f\"{OUTPUT}/trained_best{prefix}.pth\")\n",
        "                    valid_str += \" updated\"\n",
        "\n",
        "                func = logging.info\n",
        "\n",
        "            loss_sch = (cum_loss + cum_loss_align) / epoch_since\n",
        "            func(f\"train #{epoch} lr = {'%.2e' % opt.param_groups[0]['lr']} loss = {'%.6lf / %.6lf' % (cum_loss / epoch_since, cum_loss_align / epoch_since / align_coef_sum)} train = {'%.6lf' % (correct / total)} {valid_str}\")\n",
        "\n",
        "            # step here to make output beautiful\n",
        "            if epoch > warmup and epoch % print_epoch == 0:\n",
        "                sch.step()\n",
        "\n",
        "            epoch_since = cum_loss = cum_loss_align = correct = total = 0\n",
        "\n",
        "            if stop_training:\n",
        "                break\n",
        "\n",
        "        \n",
        "\n",
        "        if epoch % save_epoch == 0:\n",
        "            if not test_as_valid:\n",
        "                save(epoch)\n",
        "            tres = best_vres if test_as_valid else evaluate(model, linear, test_loader, noprint=True)\n",
        "            logging.info(f\"Saved{'(Skipped)' if test_as_valid else ''} test = {'%.6lf' % tres}\")\n",
        "\n",
        "        if epoch % cut_epoch == 0:\n",
        "            if batch_size > 8:\n",
        "                batch_size //= 2\n",
        "                epoch_scale *= 2\n",
        "\n",
        "            logging.info(f\"Cut batch_size = {batch_size} epoch_scale = {epoch_scale}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loop"
      ],
      "metadata": {
        "id": "IbtQ22wxRlMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "id": "hqQdhhWXadpx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}